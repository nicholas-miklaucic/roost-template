{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import rho_plus as rp\n",
    "\n",
    "theme, cs = rp.mpl_setup(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Null</th>\n",
       "      <th>H</th>\n",
       "      <th>He</th>\n",
       "      <th>Li</th>\n",
       "      <th>Be</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>F</th>\n",
       "      <th>...</th>\n",
       "      <th>At</th>\n",
       "      <th>Rn</th>\n",
       "      <th>Fr</th>\n",
       "      <th>Ra</th>\n",
       "      <th>Ac</th>\n",
       "      <th>Th</th>\n",
       "      <th>Pa</th>\n",
       "      <th>U</th>\n",
       "      <th>Np</th>\n",
       "      <th>Pu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.044911</td>\n",
       "      <td>0.352363</td>\n",
       "      <td>-0.067220</td>\n",
       "      <td>-0.161449</td>\n",
       "      <td>-0.111666</td>\n",
       "      <td>0.260108</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.611494</td>\n",
       "      <td>-0.113972</td>\n",
       "      <td>-0.308105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013335</td>\n",
       "      <td>-0.011944</td>\n",
       "      <td>-0.043450</td>\n",
       "      <td>0.016968</td>\n",
       "      <td>-0.918221</td>\n",
       "      <td>-0.353667</td>\n",
       "      <td>-0.498012</td>\n",
       "      <td>-0.061629</td>\n",
       "      <td>-0.471240</td>\n",
       "      <td>-0.278194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004152</td>\n",
       "      <td>0.635952</td>\n",
       "      <td>0.141113</td>\n",
       "      <td>0.179496</td>\n",
       "      <td>0.760182</td>\n",
       "      <td>0.707898</td>\n",
       "      <td>0.744485</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>-0.188673</td>\n",
       "      <td>-0.575614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024167</td>\n",
       "      <td>0.017634</td>\n",
       "      <td>0.042720</td>\n",
       "      <td>-0.025037</td>\n",
       "      <td>0.073024</td>\n",
       "      <td>-0.067413</td>\n",
       "      <td>0.487307</td>\n",
       "      <td>0.243641</td>\n",
       "      <td>0.448124</td>\n",
       "      <td>0.044107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012933</td>\n",
       "      <td>0.217338</td>\n",
       "      <td>0.164495</td>\n",
       "      <td>-0.114184</td>\n",
       "      <td>0.057829</td>\n",
       "      <td>0.064846</td>\n",
       "      <td>0.662636</td>\n",
       "      <td>0.620457</td>\n",
       "      <td>0.108998</td>\n",
       "      <td>-0.171835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009026</td>\n",
       "      <td>-0.005495</td>\n",
       "      <td>-0.044104</td>\n",
       "      <td>0.031968</td>\n",
       "      <td>-0.595052</td>\n",
       "      <td>-0.824566</td>\n",
       "      <td>-0.745676</td>\n",
       "      <td>-0.346598</td>\n",
       "      <td>-0.170911</td>\n",
       "      <td>-0.154820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.010163</td>\n",
       "      <td>-0.191956</td>\n",
       "      <td>0.136701</td>\n",
       "      <td>0.136510</td>\n",
       "      <td>0.250147</td>\n",
       "      <td>-0.300478</td>\n",
       "      <td>-0.520578</td>\n",
       "      <td>-0.446009</td>\n",
       "      <td>0.214548</td>\n",
       "      <td>0.462324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046445</td>\n",
       "      <td>-0.009977</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>-0.017894</td>\n",
       "      <td>0.450472</td>\n",
       "      <td>-0.025612</td>\n",
       "      <td>-0.009069</td>\n",
       "      <td>0.056789</td>\n",
       "      <td>0.299127</td>\n",
       "      <td>0.376608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007606</td>\n",
       "      <td>0.253751</td>\n",
       "      <td>0.016505</td>\n",
       "      <td>0.106477</td>\n",
       "      <td>-0.396934</td>\n",
       "      <td>-0.510219</td>\n",
       "      <td>-0.673885</td>\n",
       "      <td>0.094431</td>\n",
       "      <td>0.371144</td>\n",
       "      <td>0.702146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>-0.030206</td>\n",
       "      <td>-0.023090</td>\n",
       "      <td>-0.027510</td>\n",
       "      <td>-0.345009</td>\n",
       "      <td>-0.346526</td>\n",
       "      <td>-0.263530</td>\n",
       "      <td>-0.442932</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>-0.445192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.029269</td>\n",
       "      <td>-0.423261</td>\n",
       "      <td>0.073929</td>\n",
       "      <td>0.047109</td>\n",
       "      <td>0.128850</td>\n",
       "      <td>0.082544</td>\n",
       "      <td>-0.476654</td>\n",
       "      <td>-0.817922</td>\n",
       "      <td>-0.103028</td>\n",
       "      <td>0.220106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030743</td>\n",
       "      <td>-0.048440</td>\n",
       "      <td>0.029772</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>0.607715</td>\n",
       "      <td>0.809945</td>\n",
       "      <td>0.749626</td>\n",
       "      <td>0.640021</td>\n",
       "      <td>0.501166</td>\n",
       "      <td>0.493097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.033980</td>\n",
       "      <td>0.221297</td>\n",
       "      <td>0.151093</td>\n",
       "      <td>0.065104</td>\n",
       "      <td>0.037942</td>\n",
       "      <td>0.589792</td>\n",
       "      <td>0.698720</td>\n",
       "      <td>-0.130236</td>\n",
       "      <td>-0.404307</td>\n",
       "      <td>-0.503306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014643</td>\n",
       "      <td>-0.007266</td>\n",
       "      <td>0.023424</td>\n",
       "      <td>-0.024473</td>\n",
       "      <td>-0.072247</td>\n",
       "      <td>0.161332</td>\n",
       "      <td>0.464819</td>\n",
       "      <td>0.237543</td>\n",
       "      <td>0.197754</td>\n",
       "      <td>-0.023476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.018202</td>\n",
       "      <td>-0.452411</td>\n",
       "      <td>-0.138170</td>\n",
       "      <td>-0.069099</td>\n",
       "      <td>0.135463</td>\n",
       "      <td>0.523288</td>\n",
       "      <td>0.607385</td>\n",
       "      <td>-0.645908</td>\n",
       "      <td>-0.502800</td>\n",
       "      <td>-0.394980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016754</td>\n",
       "      <td>-0.025606</td>\n",
       "      <td>-0.033319</td>\n",
       "      <td>0.042808</td>\n",
       "      <td>0.179490</td>\n",
       "      <td>0.469464</td>\n",
       "      <td>0.553796</td>\n",
       "      <td>0.706783</td>\n",
       "      <td>0.673036</td>\n",
       "      <td>0.487316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.042308</td>\n",
       "      <td>-1.007713</td>\n",
       "      <td>-0.180052</td>\n",
       "      <td>-0.210581</td>\n",
       "      <td>-0.202221</td>\n",
       "      <td>-0.672266</td>\n",
       "      <td>-0.562160</td>\n",
       "      <td>0.021001</td>\n",
       "      <td>0.313225</td>\n",
       "      <td>0.283450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046804</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>0.038471</td>\n",
       "      <td>-0.026307</td>\n",
       "      <td>0.290643</td>\n",
       "      <td>0.045801</td>\n",
       "      <td>-0.281812</td>\n",
       "      <td>0.170462</td>\n",
       "      <td>0.243258</td>\n",
       "      <td>0.303537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.019320</td>\n",
       "      <td>-0.289936</td>\n",
       "      <td>-0.130642</td>\n",
       "      <td>-0.030035</td>\n",
       "      <td>-0.046165</td>\n",
       "      <td>0.490484</td>\n",
       "      <td>0.673558</td>\n",
       "      <td>-0.271362</td>\n",
       "      <td>-0.465547</td>\n",
       "      <td>-0.584290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.044316</td>\n",
       "      <td>-0.016308</td>\n",
       "      <td>-0.023788</td>\n",
       "      <td>0.057812</td>\n",
       "      <td>0.178863</td>\n",
       "      <td>0.412992</td>\n",
       "      <td>0.436729</td>\n",
       "      <td>0.359024</td>\n",
       "      <td>0.260558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.035417</td>\n",
       "      <td>0.126820</td>\n",
       "      <td>0.184020</td>\n",
       "      <td>0.122461</td>\n",
       "      <td>0.166105</td>\n",
       "      <td>0.496226</td>\n",
       "      <td>0.587440</td>\n",
       "      <td>-0.204580</td>\n",
       "      <td>-0.369454</td>\n",
       "      <td>-0.465051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023669</td>\n",
       "      <td>-0.036176</td>\n",
       "      <td>-0.033892</td>\n",
       "      <td>-0.014349</td>\n",
       "      <td>0.026626</td>\n",
       "      <td>0.044975</td>\n",
       "      <td>0.227212</td>\n",
       "      <td>0.191229</td>\n",
       "      <td>0.193990</td>\n",
       "      <td>0.159681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.020450</td>\n",
       "      <td>-0.025928</td>\n",
       "      <td>-0.024526</td>\n",
       "      <td>-0.388679</td>\n",
       "      <td>0.214563</td>\n",
       "      <td>0.386530</td>\n",
       "      <td>0.619206</td>\n",
       "      <td>0.586720</td>\n",
       "      <td>-0.103702</td>\n",
       "      <td>-0.447120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030561</td>\n",
       "      <td>0.017097</td>\n",
       "      <td>-0.041319</td>\n",
       "      <td>-0.004567</td>\n",
       "      <td>-0.726572</td>\n",
       "      <td>-0.331488</td>\n",
       "      <td>-0.476999</td>\n",
       "      <td>-0.102397</td>\n",
       "      <td>-0.122534</td>\n",
       "      <td>-0.003404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.016479</td>\n",
       "      <td>0.717508</td>\n",
       "      <td>-0.034060</td>\n",
       "      <td>0.137059</td>\n",
       "      <td>-0.021428</td>\n",
       "      <td>0.181043</td>\n",
       "      <td>0.615198</td>\n",
       "      <td>0.663307</td>\n",
       "      <td>-0.042888</td>\n",
       "      <td>-0.242727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014550</td>\n",
       "      <td>-0.031811</td>\n",
       "      <td>0.037754</td>\n",
       "      <td>-0.039498</td>\n",
       "      <td>-0.528816</td>\n",
       "      <td>-0.506210</td>\n",
       "      <td>-0.240836</td>\n",
       "      <td>-0.311334</td>\n",
       "      <td>-0.271932</td>\n",
       "      <td>-0.547764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.027975</td>\n",
       "      <td>-0.631993</td>\n",
       "      <td>-0.217201</td>\n",
       "      <td>-0.357076</td>\n",
       "      <td>-0.594812</td>\n",
       "      <td>-0.620111</td>\n",
       "      <td>-0.749764</td>\n",
       "      <td>-0.584931</td>\n",
       "      <td>0.087557</td>\n",
       "      <td>0.444719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.023763</td>\n",
       "      <td>-0.021917</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.256403</td>\n",
       "      <td>0.303512</td>\n",
       "      <td>0.183092</td>\n",
       "      <td>0.022910</td>\n",
       "      <td>0.099960</td>\n",
       "      <td>0.175796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.034489</td>\n",
       "      <td>0.021095</td>\n",
       "      <td>0.165278</td>\n",
       "      <td>0.299678</td>\n",
       "      <td>0.395654</td>\n",
       "      <td>-0.177121</td>\n",
       "      <td>-0.552250</td>\n",
       "      <td>-0.367534</td>\n",
       "      <td>0.192867</td>\n",
       "      <td>0.501850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015186</td>\n",
       "      <td>0.028262</td>\n",
       "      <td>0.011680</td>\n",
       "      <td>-0.045962</td>\n",
       "      <td>0.173583</td>\n",
       "      <td>0.069853</td>\n",
       "      <td>-0.205466</td>\n",
       "      <td>-0.080311</td>\n",
       "      <td>-0.031972</td>\n",
       "      <td>0.037212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.021560</td>\n",
       "      <td>-0.270104</td>\n",
       "      <td>-0.042185</td>\n",
       "      <td>0.306350</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>-0.171613</td>\n",
       "      <td>-0.652751</td>\n",
       "      <td>-0.559885</td>\n",
       "      <td>0.038902</td>\n",
       "      <td>0.371508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>0.044606</td>\n",
       "      <td>0.045129</td>\n",
       "      <td>-0.016306</td>\n",
       "      <td>0.606694</td>\n",
       "      <td>0.468920</td>\n",
       "      <td>0.432984</td>\n",
       "      <td>0.210230</td>\n",
       "      <td>0.123560</td>\n",
       "      <td>0.125202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Null         H        He        Li        Be         B         C  \\\n",
       "0  -0.044911  0.352363 -0.067220 -0.161449 -0.111666  0.260108  0.398148   \n",
       "1   0.004152  0.635952  0.141113  0.179496  0.760182  0.707898  0.744485   \n",
       "2   0.012933  0.217338  0.164495 -0.114184  0.057829  0.064846  0.662636   \n",
       "3  -0.010163 -0.191956  0.136701  0.136510  0.250147 -0.300478 -0.520578   \n",
       "4   0.007606  0.253751  0.016505  0.106477 -0.396934 -0.510219 -0.673885   \n",
       "5   0.029269 -0.423261  0.073929  0.047109  0.128850  0.082544 -0.476654   \n",
       "6   0.033980  0.221297  0.151093  0.065104  0.037942  0.589792  0.698720   \n",
       "7   0.018202 -0.452411 -0.138170 -0.069099  0.135463  0.523288  0.607385   \n",
       "8   0.042308 -1.007713 -0.180052 -0.210581 -0.202221 -0.672266 -0.562160   \n",
       "9  -0.019320 -0.289936 -0.130642 -0.030035 -0.046165  0.490484  0.673558   \n",
       "10 -0.035417  0.126820  0.184020  0.122461  0.166105  0.496226  0.587440   \n",
       "11  0.020450 -0.025928 -0.024526 -0.388679  0.214563  0.386530  0.619206   \n",
       "12 -0.016479  0.717508 -0.034060  0.137059 -0.021428  0.181043  0.615198   \n",
       "13  0.027975 -0.631993 -0.217201 -0.357076 -0.594812 -0.620111 -0.749764   \n",
       "14  0.034489  0.021095  0.165278  0.299678  0.395654 -0.177121 -0.552250   \n",
       "15  0.021560 -0.270104 -0.042185  0.306350  0.000693 -0.171613 -0.652751   \n",
       "\n",
       "           N         O         F  ...        At        Rn        Fr        Ra  \\\n",
       "0   0.611494 -0.113972 -0.308105  ...  0.013335 -0.011944 -0.043450  0.016968   \n",
       "1   0.001810 -0.188673 -0.575614  ...  0.024167  0.017634  0.042720 -0.025037   \n",
       "2   0.620457  0.108998 -0.171835  ... -0.009026 -0.005495 -0.044104  0.031968   \n",
       "3  -0.446009  0.214548  0.462324  ...  0.046445 -0.009977  0.007274 -0.017894   \n",
       "4   0.094431  0.371144  0.702146  ...  0.003559 -0.030206 -0.023090 -0.027510   \n",
       "5  -0.817922 -0.103028  0.220106  ...  0.030743 -0.048440  0.029772  0.013903   \n",
       "6  -0.130236 -0.404307 -0.503306  ... -0.014643 -0.007266  0.023424 -0.024473   \n",
       "7  -0.645908 -0.502800 -0.394980  ... -0.016754 -0.025606 -0.033319  0.042808   \n",
       "8   0.021001  0.313225  0.283450  ...  0.046804  0.004872  0.038471 -0.026307   \n",
       "9  -0.271362 -0.465547 -0.584290  ...  0.003551  0.044316 -0.016308 -0.023788   \n",
       "10 -0.204580 -0.369454 -0.465051  ...  0.023669 -0.036176 -0.033892 -0.014349   \n",
       "11  0.586720 -0.103702 -0.447120  ...  0.030561  0.017097 -0.041319 -0.004567   \n",
       "12  0.663307 -0.042888 -0.242727  ...  0.014550 -0.031811  0.037754 -0.039498   \n",
       "13 -0.584931  0.087557  0.444719  ...  0.003774  0.023763 -0.021917  0.003126   \n",
       "14 -0.367534  0.192867  0.501850  ...  0.015186  0.028262  0.011680 -0.045962   \n",
       "15 -0.559885  0.038902  0.371508  ...  0.005133  0.044606  0.045129 -0.016306   \n",
       "\n",
       "          Ac        Th        Pa         U        Np        Pu  \n",
       "0  -0.918221 -0.353667 -0.498012 -0.061629 -0.471240 -0.278194  \n",
       "1   0.073024 -0.067413  0.487307  0.243641  0.448124  0.044107  \n",
       "2  -0.595052 -0.824566 -0.745676 -0.346598 -0.170911 -0.154820  \n",
       "3   0.450472 -0.025612 -0.009069  0.056789  0.299127  0.376608  \n",
       "4  -0.345009 -0.346526 -0.263530 -0.442932 -0.490186 -0.445192  \n",
       "5   0.607715  0.809945  0.749626  0.640021  0.501166  0.493097  \n",
       "6  -0.072247  0.161332  0.464819  0.237543  0.197754 -0.023476  \n",
       "7   0.179490  0.469464  0.553796  0.706783  0.673036  0.487316  \n",
       "8   0.290643  0.045801 -0.281812  0.170462  0.243258  0.303537  \n",
       "9   0.057812  0.178863  0.412992  0.436729  0.359024  0.260558  \n",
       "10  0.026626  0.044975  0.227212  0.191229  0.193990  0.159681  \n",
       "11 -0.726572 -0.331488 -0.476999 -0.102397 -0.122534 -0.003404  \n",
       "12 -0.528816 -0.506210 -0.240836 -0.311334 -0.271932 -0.547764  \n",
       "13  0.256403  0.303512  0.183092  0.022910  0.099960  0.175796  \n",
       "14  0.173583  0.069853 -0.205466 -0.080311 -0.031972  0.037212  \n",
       "15  0.606694  0.468920  0.432984  0.210230  0.123560  0.125202  \n",
       "\n",
       "[16 rows x 95 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem_embs = pd.read_json('https://raw.githubusercontent.com/CompRhys/aviary/refs/heads/main/aviary/embeddings/element/megnet16.json')\n",
    "elem_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from aviary.roost.model import DescriptorNetwork\n",
    "from pymatgen.core import Composition\n",
    "from torch import Tensor, LongTensor\n",
    "\n",
    "def comp2graph(composition):\n",
    "    comp_dict = Composition(composition).get_el_amt_dict()\n",
    "    elements = list(comp_dict)\n",
    "\n",
    "    weights = list(comp_dict.values())\n",
    "    weights = np.atleast_2d(weights).T / np.sum(weights)\n",
    "\n",
    "    try:\n",
    "        elem_fea = np.vstack([elem_embs[elements]]).T\n",
    "    except AssertionError as exc:\n",
    "        raise AssertionError(\n",
    "            f\"{composition} contains element types not in embedding\"\n",
    "        ) from exc\n",
    "    except ValueError as exc:\n",
    "        raise ValueError(\n",
    "            f\"{composition} composition cannot be parsed into elements\"\n",
    "        ) from exc\n",
    "\n",
    "    n_elems = len(elements)\n",
    "    self_idx = []\n",
    "    nbr_idx = []\n",
    "    for elem_idx in range(n_elems):\n",
    "        self_idx += [elem_idx] * n_elems\n",
    "        nbr_idx += list(range(n_elems))\n",
    "\n",
    "    # convert all data to tensors\n",
    "    elem_weights = Tensor(weights)\n",
    "    elem_fea = Tensor(elem_fea)\n",
    "    self_idx = LongTensor(self_idx)\n",
    "    nbr_idx = LongTensor(nbr_idx)\n",
    "    return (elem_weights, elem_fea, self_idx, nbr_idx)\n",
    "\n",
    "# https://github.com/CompRhys/aviary/blob/181e2b2b2d679a12f6dbb430853d92508e8d71f2/aviary/roost/data.py#L140C1-L212C6\n",
    "def collate_batch(samples):\n",
    "    # define the lists\n",
    "    batch_elem_weights = []\n",
    "    batch_elem_fea = []\n",
    "    batch_self_idx = []\n",
    "    batch_nbr_idx = []\n",
    "    crystal_elem_idx = []\n",
    "\n",
    "    cry_base_idx = 0\n",
    "    for idx, inputs in enumerate(samples):\n",
    "        elem_weights, elem_fea, self_idx, nbr_idx = inputs\n",
    "\n",
    "        n_sites = elem_fea.shape[0]  # number of atoms for this crystal\n",
    "\n",
    "        # batch the features together\n",
    "        batch_elem_weights.append(elem_weights)\n",
    "        batch_elem_fea.append(elem_fea)\n",
    "\n",
    "        # mappings from bonds to atoms\n",
    "        batch_self_idx.append(self_idx + cry_base_idx)\n",
    "        batch_nbr_idx.append(nbr_idx + cry_base_idx)\n",
    "\n",
    "        # mapping from atoms to crystals\n",
    "        crystal_elem_idx.append(torch.tensor([idx] * n_sites))\n",
    "\n",
    "        # increment the id counter\n",
    "        cry_base_idx += n_sites\n",
    "\n",
    "    return (        \n",
    "        torch.cat(batch_elem_weights, dim=0),\n",
    "        torch.cat(batch_elem_fea, dim=0),\n",
    "        torch.cat(batch_self_idx, dim=0),\n",
    "        torch.cat(batch_nbr_idx, dim=0),\n",
    "        torch.cat(crystal_elem_idx),        \n",
    "    )\n",
    "\n",
    "class CompositionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, elem_input_dim: int = 16, elem_hidden_dim: int = 64, comp_embed_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.gnn = DescriptorNetwork(elem_emb_len=elem_input_dim, elem_fea_len=elem_hidden_dim, n_graph=1)\n",
    "        self.head = nn.Linear(elem_hidden_dim, comp_embed_dim)\n",
    "        self.rescale = nn.Parameter(torch.ones(1, dtype=torch.float32))\n",
    "\n",
    "    def embed(self, X):\n",
    "        return self.head(self.gnn(*X))\n",
    "\n",
    "    def forward(self, X1, X2):\n",
    "        z1 = self.embed(X1)\n",
    "        z2 = self.embed(X2)\n",
    "\n",
    "        dists = torch.sqrt(torch.sum(torch.square(z1 - z2), axis=1))\n",
    "        return self.to_probability(dists)\n",
    "    \n",
    "    def to_probability(self, dists):\n",
    "        return 1 - torch.tanh(dists * self.rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2821461/521401520.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('checkpoints/test.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompositionEmbedding(\n",
       "  (gnn): DescriptorNetwork(n_graph=1, cry_heads=3, elem_emb_len=16, elem_fea_len=63)\n",
       "  (head): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('checkpoints/test.pt')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = pd.read_csv('https://raw.githubusercontent.com/usccolumbia/cspbenchmark/main/data/CSPbenchmark_test_data.csv')\n",
    "benchmark_ids = benchmark['material_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(comp_1: str, other_comps: list[str], batch_size: int = 32):\n",
    "    X1 = collate_batch([comp2graph(comp_1)])\n",
    "    X2 = []\n",
    "    for i in range(0, len(other_comps), batch_size):\n",
    "        X2.append(collate_batch([comp2graph(c) for c in other_comps.iloc[i:i+batch_size]]))\n",
    "\n",
    "    model.eval()\n",
    "    z1 = model.embed(X1)\n",
    "    z2 = torch.cat([model.embed(x) for x in X2])\n",
    "\n",
    "    probs = model.to_probability(torch.cdist(z1, z2).reshape(-1))\n",
    "    return probs.numpy(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18, 0.26, 0.05, 0.06, 0.  , 0.11, 0.18, 0.06, 0.02, 0.03, 0.01,\n",
       "       0.  , 0.1 , 0.04, 0.03, 0.07, 0.06, 0.03, 0.01, 0.22, 0.04, 0.12,\n",
       "       0.  , 0.06, 0.04, 0.02, 0.13, 0.05, 0.03, 0.08, 0.08, 0.12, 0.07,\n",
       "       0.05, 0.1 , 1.  , 0.02, 0.06, 0.04, 0.03, 0.07, 0.  , 0.04, 0.02,\n",
       "       0.  , 0.  , 0.03, 0.  , 0.13, 0.02, 0.  , 0.  , 0.  , 0.01, 0.  ,\n",
       "       0.05, 0.  , 0.08, 0.18, 0.  , 0.04, 0.05, 0.04, 0.02, 0.  , 0.  ,\n",
       "       0.02, 0.02, 0.03, 0.02, 0.03, 0.  , 0.02, 0.02, 0.  , 0.01, 0.04,\n",
       "       0.01, 0.09, 0.04, 0.11, 0.01, 0.  , 0.08, 0.01, 0.01, 0.13, 0.13,\n",
       "       0.04, 0.06, 0.19, 0.1 , 0.22, 0.04, 0.03, 0.04, 0.04, 0.04, 0.03,\n",
       "       0.03, 0.03, 0.03, 0.01, 0.03, 0.02, 0.03, 0.02, 0.03, 0.04, 0.  ,\n",
       "       0.04, 0.01, 0.02, 0.04, 0.01, 0.  , 0.02, 0.  , 0.02, 0.  , 0.02,\n",
       "       0.02, 0.03, 0.  , 0.01, 0.15, 0.06, 0.01, 0.  , 0.15, 0.02, 0.  ,\n",
       "       0.01, 0.  , 0.04, 0.  , 0.  , 0.  , 0.  , 0.06, 0.19, 0.25, 0.03,\n",
       "       0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.17, 0.  , 0.01, 0.22, 0.  ,\n",
       "       0.02, 0.  , 0.  , 0.  , 0.14, 0.  , 0.01, 0.03, 0.25, 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.05,\n",
       "       0.03, 0.  , 0.02, 0.  ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = compute_scores('Nb3Si', benchmark['full_formula'])\n",
    "scores.round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baysic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
